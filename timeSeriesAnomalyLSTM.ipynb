{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPRqf+K+64zrya9WiTI3EbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nomadyb/AttritionPredict/blob/main/timeSeriesAnomalyLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Anomaly Detection </h1>\n",
        "<p>\n",
        "\n",
        "  Anomali tespiti nadir olaylarınbulunmasını ve tanımlanmasını ifade eder.\n",
        "  Bazı uygulamalar arasında banka dolandıcılığı tespiti,tıbbi görüntülemede tümör tespiti ve yazılı metindeki hatalar yer alır.\n",
        "  Denetimli ve denetimsiz bir çok yaklaşım kullanılır.\n",
        "  <br>\n",
        "   **Tek sınıf DVM'ler**\n",
        "  <br>\n",
        "   **Bayes Ağları**\n",
        "  <br>\n",
        "   **küme Analizi**\n",
        "  <br>\n",
        "   **Nöral Ağlar**\n",
        "  <br>\n",
        "\n",
        "  S&P 500 endeksindeki anomalileri tespit etmek için bir LSTM autencoder Siner Ağı kullanılmıştır.\n",
        "</p>\n",
        "\n",
        "<h3>Yeni veri noktası için hata (yeniden yapılandırma hatası) belirli bir eşiğin üzerindeyse, örneği anomali olarak etiketleriz<h3>\n",
        "\n"
      ],
      "metadata": {
        "id": "rhPoEYjm2nAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EMzIaEr2A8m"
      },
      "outputs": [],
      "source": [
        "!gdown --id 10vdMg_RazoIatwrT7azKFX4P02OebU76 --output spx.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "3Hp4ha9n75bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"spx.csv\",parse_dates=[\"date\"],index_col=\"date\")\n",
        "df\n"
      ],
      "metadata": {
        "id": "iX1Sxz_b7pdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing\n"
      ],
      "metadata": {
        "id": "J6T2xlyj86Tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.iloc[0:train_size] -> ilk girişten itibaren train_size indesine kadar olan bölümü alır. ve train adında yeni bir dataframe oluşturur.\n",
        "\n",
        "df.iloc[train_size:len(df)] -> train_Size indeksinden dataframein sonuna kadar olan bölümü alır ve test frame'ine dönüştürür.\n",
        "\n",
        "train_Size değerimizi df'in %95 olarak tanımladık.\n",
        "Yani train kısmı %95 iken, test kısmı %5 olur.\n",
        "overfitting ihtimalini azaltmak için böyle bir tercihte bulunuruz"
      ],
      "metadata": {
        "id": "05VUD3Z0-Bjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(df)*0.95)\n",
        "test_size = len(df)- train_size\n",
        "train,test = df.iloc[0:train_size],df.iloc[train_size:len(df)]\n",
        "print(train.shape,test.shape)\n"
      ],
      "metadata": {
        "id": "NR8JBZRs86FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "şimdi eğitim verilerini kullanarak verileri yeniden ölçeklendireceğiz ve aynı dönüşümü test verilerine uygulayacağız.\n",
        "\n",
        "\n",
        "> Ölçeklendirme modellerin daha iyi sonuç vermesini sağlar.Amacımız verilerdeki büyük farklılıklar olduğunda modelimizde oluşabilecek problemleri ortadan kaldırmaktır. \"StandarScaler\" her bir özelliğin ortalama değerini 0 ve standart sapmasını 1 olacak şekilde ayarlar.Fit fonksiyonu \"train\" veri setindeki \"close\" özelliği üzerinde hesaplama yapar ve bu hesaplama sonucunda öğrenilen parametreleri (ortalama ve standart sapma ) içinde saklar.\n",
        "Sonrasında \"transform\" fonksiyonu bu öğrenilen parametrelerle \"train\" ve test\" veri setindeki \"close özelliğini normalize eder. bu şekilde uyum hedeflenir.\n",
        "\n"
      ],
      "metadata": {
        "id": "jIsDBfkHAhK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "QQhdOQx-BFl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(train[['close']])\n",
        "train['close'] = scaler.transform(train[['close']])\n",
        "test['close'] = scaler.transform(test[['close']])"
      ],
      "metadata": {
        "id": "clXU1-tnAoKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Şimdi verilerin Alt dizelere ayrılması sağlanır.\n",
        "\n",
        "---\n",
        "\n",
        "Bu fonksiyonun ana maacı bir zaman serisi veri setinden özellikler ve etiketler oluşturmayı sağlamaktır.Giriş olarak X(bağımsız değişkenler) ve y(bağımlı değişkenler) alır ve \"time_steps\" kadar önceki gözlemlerle yeni bir veri seti oluşturur.Bu tip veri düzenlemesi genellikle zaman serisi tahmin problemlerinde , özellikle de derin öğrenme tabanlı modellemelerde LONG Short Term Memory kullanılır."
      ],
      "metadata": {
        "id": "hZ3OdzQnC3Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "YnAiMS-mGb_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy  as np\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        Xs.append(v)\n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "#bu fonksiyon list compheresion yöntemi ile yazılabilir."
      ],
      "metadata": {
        "id": "bAGfOL_7C8Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_compher(X,y,time_steps=1):\n",
        "  Xs = [X.iloc[i:i+ time_steps ].values for i in range (len(X)-time_steps)]\n",
        "  ys = [y.iloc[i+time_steps] for i in range(len(y) - time_steps)]\n",
        "  return np.array(Xs),np.array(ys)\n",
        ""
      ],
      "metadata": {
        "id": "hA2C8k6ME4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Şimdi memory yapısı için geçmişe dönük 30 günlük veri içeren sekanslar oluşturacağız.\n",
        "\n",
        "create_dataset fonksiyonu kullanılmaktadır\n",
        "Zaman serisi için test ve eğitim test verileri oluşturulmaktadırç\n",
        "Fonksiyon 4 değişken için çıktı üretir.\n",
        "Boyutu TIME_STEPS kadar olur."
      ],
      "metadata": {
        "id": "wLOPkHcmFqIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TIME_STEPS = 30\n",
        "\n",
        "X_train,y_train = create_dataset(\n",
        "    train[[\"close\"]],\n",
        "    train.close,\n",
        "    TIME_STEPS\n",
        ")\n",
        "\n",
        "\n",
        "X_test,y_test = create_dataset(\n",
        "    test[[\"close\"]],\n",
        "    test.close,\n",
        "    TIME_STEPS\n",
        ")\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2NgqT_W8FxT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM autoencoder in Keras\n",
        "\n",
        "\n",
        "---\n",
        "**Burada amacımız LSTM nöral ağ yapısını oluşturmaktır.**\n",
        "<br>\n",
        "**1.katman LSTM katmandır units parametresi LSTM katmanındaki nöral sayısını belirtir.**\n",
        "<br>\n",
        "**2.katman dropout yapar bu ağın ezberleme yani overfitting olasılığını azaltmak için kullanılır. rate=0.2 parametresi her eğtim için rastgele olarak close olarak belirlenen nöron oranını belirtir.**\n",
        "<br>\n",
        "**3.katman bir repeatvector katmanıdır.Burada önceki katmanın çıktısını belirli sayıda tekrarlama yapılır eğer sonraki katman bir girdi alıyorsa yaralıdır.**\n",
        "<br>\n",
        "**4.katman başka bir LSTM katmanıdır. burada return_sequences=True parametresi LSTM katmanının her time serisinde çıktı üretmesini sağlar overfitting önlemeyi amaçlar.**\n",
        "<br>\n",
        "**6.katman ise TimeDistributed yapar bir dense (tam bağlantılı) katmanı tüm time series adımlarında uygular**\n",
        "<br>\n",
        "**Son olarak \"model.compile(loss=\"mae\", optimiez=\"adam\") modeli derler.Modelin eğitilmesinden önce gerçekleşir. \"loss\"=\"mae\" parametresi gerçekleşen hataları ölçmek için kullanılcak fonksiyonu belirtir. \"optimizer\"=\"adam\" parametresi modelin ağırlıklarını güncellemek için kullanacağı optimizasyon algoritmasını belirtir.  **"
      ],
      "metadata": {
        "id": "7FiC21c2IHfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "25h64sqjIMV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.LSTM(\n",
        "    units=64,\n",
        "    input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "))\n",
        "\n",
        "\n",
        "\n",
        "model.add(keras.layers.Dropout(rate=0.2))\n",
        "model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
        "model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
        "model.add(keras.layers.Dropout(rate=0.2))\n",
        "model.add(\n",
        "  keras.layers.TimeDistributed(\n",
        "    keras.layers.Dense(units=X_train.shape[2])\n",
        "\n",
        "\n",
        "\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='mae', optimizer='adam')"
      ],
      "metadata": {
        "id": "JhW3fv3SMW2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_train , y_train :eğtim verisi ve karşılık gelen etiketlerimizdir.Modelimizde amacımız X_train verisine göre y_train labelllarını tahmin etmektir.\n",
        "<br>\n",
        "<br>\n",
        "epochs=10:Modelin eiğtim verileri üzerinde kaç kez geçiş yapılacağını belirler.\n",
        "<br>\n",
        "<br>\n",
        "batch_size=32 Model her güncelleme adımında 32 örneği aynı anda işlemektedir.size büyüdükçe eğtiim hızlanır ama  ama bellek ihtiyacı artar.\n",
        "<br>\n",
        "<br>\n",
        "validation_split=0.1 eğitim verilerini %10'unu doğrulama yani validation seti olarak ayırır.Bu modelin eğtim sürecindeki performansını izlemek için kullanılır.Model bu doğrulama verilerini eğitimde kullanmaz.Bu veri seti üzerindeki performansı sadece epoch sonunda değerlendirir.\n",
        "<br>\n",
        "<br>\n",
        "shuffle=False eğitim verilerinin her epoch başında karıştırılıp karıştırılmayacağını belirler.Time Series  tahmini gibi sıralı verilerle çalışırken shuffle=False olarak kullanılırç\n",
        "amacımız history çıktısını yani eğitim sürecini değerlendirmektir."
      ],
      "metadata": {
        "id": "dGdlcts7QB_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "cxVaaYR9QNUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "chpduzB5SdOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "o8wXENvUSrhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model loss progress during training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EkM1gA1yStmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "\n",
        "print(\"Training Loss: \", training_loss)\n",
        "print(\"Validation Loss: \", validation_loss)\n"
      ],
      "metadata": {
        "id": "pMOSNOQCT_jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1466NqdUI2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomali bulma\n",
        "*Amaç X_train veri setindeki tahminleri alıp daha sonra bunların gerçek değerler ile ne kadar farklı olduğunu hesaplamaktır Bu farkın mutlak değerinin ortalamasına \"mae\" yani Mean Absolute Error denir.*\n",
        "bununla ilgili bir histogram(veri dağılım grafiği çizelim)"
      ],
      "metadata": {
        "id": "I1khkMVmUT-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pred = model.predict(X_train)\n",
        "train_mae_loss = np.mean(np.abs(X_train_pred - X_train),axis=1)\n"
      ],
      "metadata": {
        "id": "251jiNsGUZll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(train_mae_loss, bins=50)\n",
        "plt.xlabel(\"Train MAE loss\")\n",
        "plt.ylabel(\"Number of Samples\");\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q7VLTsurVDkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_mae_loss)\n",
        "plt.xlabel(\"Sample index\")\n",
        "plt.ylabel(\"Train MAE loss\");\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n7RFUs3MVbuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eşik Değer seçimi"
      ],
      "metadata": {
        "id": "-6j96n3DVs58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.65"
      ],
      "metadata": {
        "id": "Tpuui0yfV5yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pred = model.predict(X_test)\n",
        "test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)"
      ],
      "metadata": {
        "id": "X3KIHfo3WWsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
        "test_score_df['loss'] = test_mae_loss\n",
        "test_score_df['threshold'] = THRESHOLD\n",
        "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
        "test_score_df['close'] = test[TIME_STEPS:].close"
      ],
      "metadata": {
        "id": "A785Aw-3WbC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
        "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold', color='r')\n",
        "plt.xticks(rotation=10)\n",
        "plt.title('Test Loss vs. Threshold')\n",
        "plt.legend();\n"
      ],
      "metadata": {
        "id": "AEUKzbuCWhTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies = test_score_df[test_score_df.anomaly == True]"
      ],
      "metadata": {
        "id": "kWzppDfSXD4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "plt.plot(test_score_df.index, test_score_df['close'], label='close')\n",
        "\n",
        "\n",
        "anomalies = test_score_df[test_score_df.anomaly == True]\n",
        "\n",
        "\n",
        "plt.scatter(anomalies.index, anomalies.close, color='r', label='Anomaly')\n",
        "\n",
        "plt.xticks(rotation=25)\n",
        "plt.title('Detected Anomalies in the Test Data')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YRUabKfnXTSC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}